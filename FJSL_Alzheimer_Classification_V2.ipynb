{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colab Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import shutil\n",
    "import os\n",
    " \n",
    "FILE_NAME = \"Data.zip\"\n",
    " \n",
    "def copy_zip_file(src_path, dest_dir):\n",
    " \n",
    "    zip_filename = os.path.basename(src_path)\n",
    "    dest_path = os.path.join(dest_dir, zip_filename)\n",
    " \n",
    "    if not os.path.exists(src_path):\n",
    "        print(f\"Error: The file '{src_path}' does not exist.\")\n",
    "        return\n",
    " \n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.makedirs(dest_dir)\n",
    " \n",
    "    shutil.copy2(src_path, dest_path)\n",
    "    print(f\"'{zip_filename}' has been copied to '{dest_dir}'.\")\n",
    " \n",
    "source_path = \"/content/drive/MyDrive/\" + FILE_NAME\n",
    "destination_directory = \"/content\"\n",
    " \n",
    "copy_zip_file(source_path, destination_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!unzip Data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if tf.test.gpu_device_name(): \n",
    "\n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "\n",
    "else:\n",
    "\n",
    "   print(\"Please install GPU version of TF\")\n",
    "\n",
    "print(f\"-> {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "import random\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Paths\n",
    "dataset_dir = \"Data\"  # Root folder containing class folders\n",
    "# Adjust class names to match actual folder names\n",
    "classes = [\"Mild Dementia\", \"Moderate Dementia\", \"Non Demented\", \"Very mild Dementia\"]\n",
    "\n",
    "# Load Data with Correct Folder Names\n",
    "image_paths, labels = [], []\n",
    "for class_label, class_name in enumerate(classes):\n",
    "    class_dir = os.path.join(dataset_dir, class_name)\n",
    "    if not os.path.exists(class_dir):\n",
    "        print(f\"Error: Folder {class_dir} does not exist.\")\n",
    "        continue\n",
    "    files = glob.glob(f\"{class_dir}/*.jpg\")  # Adjust extension if needed\n",
    "    print(f\"Class: {class_name}, Files Found: {len(files)}\")  # Debug: Count files\n",
    "    for file_path in files:\n",
    "        image_paths.append(file_path)\n",
    "        labels.append(class_label)\n",
    "\n",
    "# Proceed with the pipeline if files are found\n",
    "if len(image_paths) == 0:\n",
    "    raise ValueError(\"No images found. Check dataset folder names or file paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [128, 128])\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize to [0, 1]\n",
    "    return image\n",
    "\n",
    "# Load Dataset\n",
    "def load_dataset(image_paths, labels):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    dataset = dataset.map(lambda x, y: (preprocess_image(x), y))\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing so all datasets contain image tensors (not paths)\n",
    "preprocessed_dataset = load_dataset(image_paths, labels)\n",
    "\n",
    "# Split by class\n",
    "class_2_dataset = preprocessed_dataset.filter(lambda img, label: tf.equal(label, 2)).shuffle(buffer_size=1000).take(15_000)\n",
    "class_3_dataset = preprocessed_dataset.filter(lambda img, label: tf.equal(label, 3))\n",
    "class_0_dataset = preprocessed_dataset.filter(lambda img, label: tf.equal(label, 0))\n",
    "class_1_dataset = preprocessed_dataset.filter(lambda img, label: tf.equal(label, 1))\n",
    "\n",
    "# Combine the datasets\n",
    "final_dataset = (class_2_dataset\n",
    "                 .concatenate(class_3_dataset)\n",
    "                 .concatenate(class_0_dataset)\n",
    "                 .concatenate(class_1_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_counts = Counter()\n",
    "\n",
    "# Loop through dataset\n",
    "for _, lbl in final_dataset:\n",
    "    class_index = int(lbl.numpy())  # Convert one-hot to class index\n",
    "    label_counts[class_index] += 1\n",
    "\n",
    "# Print class distribution\n",
    "print(\"Final dataset class distribution:\")\n",
    "for label, count in sorted(label_counts.items()):\n",
    "    print(f\"Class {label}: {count} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def save_dataset_to_folder(dataset, folder_path, class_names):\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    for class_name in class_names:\n",
    "        class_dir = os.path.join(folder_path, class_name)\n",
    "        os.makedirs(class_dir, exist_ok=True)\n",
    "\n",
    "    counter = [0] * len(class_names)  # To count how many images per class\n",
    "\n",
    "    for image, label in dataset:\n",
    "        # If label is one-hot encoded, convert to integer\n",
    "        if tf.rank(label) > 0:\n",
    "            label = tf.argmax(label, axis=-1)\n",
    "\n",
    "        print(label)\n",
    "        label = int(label.numpy())\n",
    "        class_name = class_names[label]\n",
    "\n",
    "        # Convert tensor to numpy and save as PNG\n",
    "        image_np = image.numpy()\n",
    "        if image_np.dtype != 'uint8':\n",
    "            image_np = (image_np * 255).astype('uint8')  # normalize if needed\n",
    "\n",
    "        img = Image.fromarray(image_np)\n",
    "        img_path = os.path.join(folder_path, class_name, f'{counter[label]}.jpg')\n",
    "        img.save(img_path)\n",
    "\n",
    "        counter[label] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset_to_folder(final_dataset, \"final_dataset\", classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Paths\n",
    "dataset_dir = \"final_dataset\"  # Root folder containing class folders\n",
    "# Adjust class names to match actual folder names\n",
    "classes = [\"Mild Dementia\", \"Moderate Dementia\", \"Non Demented\", \"Very mild Dementia\"]\n",
    "\n",
    "# Load Data with Correct Folder Names\n",
    "image_paths, labels = [], []\n",
    "for class_label, class_name in enumerate(classes):\n",
    "    class_dir = os.path.join(dataset_dir, class_name)\n",
    "    if not os.path.exists(class_dir):\n",
    "        print(f\"Error: Folder {class_dir} does not exist.\")\n",
    "        continue\n",
    "    files = glob.glob(f\"{class_dir}/*.jpg\")  # Adjust extension if needed\n",
    "    print(f\"Class: {class_name}, Files Found: {len(files)}\")  # Debug: Count files\n",
    "    for file_path in files:\n",
    "        image_paths.append(file_path)\n",
    "        labels.append(class_label)\n",
    "\n",
    "# Proceed with the pipeline if files are found\n",
    "if len(image_paths) == 0:\n",
    "    raise ValueError(\"No images found. Check dataset folder names or file paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, testing and validation\n",
    "train_paths, test_paths, train_labels, test_labels = train_test_split(image_paths, labels, test_size=0.3, random_state=42,stratify=labels) # suffle by default and straity labels \n",
    "test_paths, val_paths, test_labels, val_labels = train_test_split(test_paths, test_labels, test_size=0.5, random_state=42,stratify=test_labels) # suffle by default and straity labels\n",
    "# to keep the same class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(train_paths, train_labels)\n",
    "val_dataset = load_dataset(val_paths, val_labels)\n",
    "test_dataset = load_dataset(test_paths, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset_to_folder(train_dataset, \"train_dataset\", classes)\n",
    "save_dataset_to_folder(val_dataset, \"val_dataset\", classes)\n",
    "save_dataset_to_folder(test_dataset, \"test_dataset\", classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Agumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Paths\n",
    "dataset_dir = \"train_dataset\"  # Root folder containing class folders\n",
    "# Adjust class names to match actual folder names\n",
    "classes = [\"Mild Dementia\", \"Moderate Dementia\", \"Non Demented\", \"Very mild Dementia\"]\n",
    "\n",
    "# Load Data with Correct Folder Names\n",
    "image_paths, labels = [], []\n",
    "for class_label, class_name in enumerate(classes):\n",
    "    class_dir = os.path.join(dataset_dir, class_name)\n",
    "    if not os.path.exists(class_dir):\n",
    "        print(f\"Error: Folder {class_dir} does not exist.\")\n",
    "        continue\n",
    "    files = glob.glob(f\"{class_dir}/*.jpg\")  # Adjust extension if needed\n",
    "    print(f\"Class: {class_name}, Files Found: {len(files)}\")  # Debug: Count files\n",
    "    for file_path in files:\n",
    "        image_paths.append(file_path)\n",
    "        labels.append(class_label)\n",
    "\n",
    "# Proceed with the pipeline if files are found\n",
    "if len(image_paths) == 0:\n",
    "    raise ValueError(\"No images found. Check dataset folder names or file paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = \"train_dataset\"\n",
    "path_val = \"val_dataset\"\n",
    "#Rescale data and create data generator instances\n",
    "# train_datagenerator = ImageDataGenerator(rescale=1/255.)\n",
    "val_datagenerator = ImageDataGenerator(rescale=1/255.)\n",
    "train_datagenerator_augmentation = ImageDataGenerator(rescale = 1/255.,\n",
    "                                                      rotation_range=20, #rotate the image\n",
    "                                                      zoom_range = 0.2,#zoom the image\n",
    "                                                      width_shift_range=0.2, #shift the image horizontally\n",
    "                                                      height_shift_range=0.2, #shift the image vertically\n",
    "                                                      horizontal_flip=True, #flip the image on horizontal axis\n",
    "                                                      vertical_flip=True, #flip the image on vertical axis\n",
    "                                                      shear_range = 0.2) #Shear the image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data in from images and turn into batches\n",
    "# train_data = train_datagenerator.flow_from_directory(path_treino,\n",
    "#                                                      target_size=(128,128),\n",
    "#                                                      batch_size=32,\n",
    "#                                                      class_mode='categorical'\n",
    "#                                                     )\n",
    "val_data = val_datagenerator.flow_from_directory(path_val,\n",
    "                                                     target_size=(128,128),\n",
    "                                                     batch_size=32,\n",
    "                                                     class_mode='categorical'\n",
    "                                                    )\n",
    "train_data_augmented = train_datagenerator_augmentation.flow_from_directory(path_train,\n",
    "                                                                            target_size=(128,128),\n",
    "                                                                            batch_size=32,\n",
    "                                                                            class_mode='categorical',\n",
    "                                                                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset_to_folder_augmented(dataset, folder_path, class_names):\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    for class_name in class_names:\n",
    "        class_dir = os.path.join(folder_path, class_name)\n",
    "        os.makedirs(class_dir, exist_ok=True)\n",
    "\n",
    "    counter = [0] * len(class_names)  # To count how many images per class\n",
    "\n",
    "    for image, label in dataset:\n",
    "        # If label is one-hot encoded, convert to integer\n",
    "        if tf.rank(label) > 0:\n",
    "            label = tf.argmax(label, axis=-1)  # This will give the class index\n",
    "\n",
    "        # Now loop over the batch of labels and process each image\n",
    "        for i in range(len(label)):\n",
    "            single_label = int(label[i].numpy())  # Convert individual label to int\n",
    "            class_name = class_names[single_label]\n",
    "\n",
    "            # Convert tensor to numpy and save as PNG\n",
    "            image_np = image[i]  # Access the i-th image in the batch (no need for .numpy())\n",
    "            if image_np.dtype != 'uint8':\n",
    "                image_np = (image_np * 255).astype('uint8')  # normalize if needed\n",
    "\n",
    "            img = Image.fromarray(image_np)\n",
    "            img_path = os.path.join(folder_path, class_name, f'{counter[single_label]}.jpg')\n",
    "            img.save(img_path)\n",
    "\n",
    "            counter[single_label] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dataset_to_folder_augmented(train_data_augmented, \"train_dataset_augmentation\", classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_counts = Counter()\n",
    "\n",
    "# # Loop through dataset (limited by steps per epoch unless infinite loop)\n",
    "# for _, lbl in train_data_augmented:\n",
    "#     class_indices = np.argmax(lbl, axis=1) \n",
    "#     label_counts.update(class_indices)\n",
    "\n",
    "#     # Optional: break if you've gone through the entire dataset once\n",
    "#     if train_data_augmented.batch_index == 0:\n",
    "#         break\n",
    "\n",
    "# # Print class distribution\n",
    "# print(\"Final dataset class distribution:\")\n",
    "# for label, count in sorted(label_counts.items()):\n",
    "#     print(f\"Class {label}: {count} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_paths_under = np.array(train_paths)\n",
    "# test_paths = np.array(test_paths)\n",
    "# val_paths_under = np.array(val_paths)\n",
    "\n",
    "# # One-hot encode the labels\n",
    "# label_binarizer = LabelBinarizer()\n",
    "# train_labels = label_binarizer.fit_transform(train_labels)\n",
    "# test_labels = label_binarizer.transform(test_labels)\n",
    "# val_labels = label_binarizer.transform(val_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "def create_cnn(num_classes=4):\n",
    "    model = keras.Sequential([\n",
    "        # Convolutional Block 1\n",
    "        layers.Conv2D(32, (3,3), activation='relu', input_shape=(128, 128, 3)),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        # Convolutional Block 2\n",
    "        layers.Conv2D(64, (3,3), activation='relu'),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        # Convolutional Block 3\n",
    "        layers.Conv2D(128, (3,3), activation='relu'),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        # Flatten & Dense Layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),  # Reduce overfitting\n",
    "        layers.Dense(num_classes, activation='softmax')  # Output layer\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        # loss='categorical_crossentropy',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "cnn_model = create_cnn()\n",
    "\n",
    "# Print model summary\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "cnn_model.fit(train_dataset, validation_data=val_dataset, epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AAUTIA2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
