{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colab Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import shutil\n",
    "import os\n",
    " \n",
    "FILE_NAME = \"Data.zip\"\n",
    " \n",
    "def copy_zip_file(src_path, dest_dir):\n",
    " \n",
    "    zip_filename = os.path.basename(src_path)\n",
    "    dest_path = os.path.join(dest_dir, zip_filename)\n",
    " \n",
    "    if not os.path.exists(src_path):\n",
    "        print(f\"Error: The file '{src_path}' does not exist.\")\n",
    "        return\n",
    " \n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.makedirs(dest_dir)\n",
    " \n",
    "    shutil.copy2(src_path, dest_path)\n",
    "    print(f\"'{zip_filename}' has been copied to '{dest_dir}'.\")\n",
    " \n",
    "source_path = \"/content/drive/MyDrive/\" + FILE_NAME\n",
    "destination_directory = \"/content\"\n",
    " \n",
    "copy_zip_file(source_path, destination_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!unzip Data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if tf.test.gpu_device_name(): \n",
    "\n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "\n",
    "else:\n",
    "\n",
    "   print(\"Please install GPU version of TF\")\n",
    "\n",
    "print(f\"-> {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "import random\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Paths\n",
    "dataset_dir = \"Data\"  # Root folder containing class folders\n",
    "# Adjust class names to match actual folder names\n",
    "classes = [\"Mild Dementia\", \"Moderate Dementia\", \"Non Demented\", \"Very mild Dementia\"]\n",
    "\n",
    "# Load Data with Correct Folder Names\n",
    "image_paths, labels = [], []\n",
    "for class_label, class_name in enumerate(classes):\n",
    "    class_dir = os.path.join(dataset_dir, class_name)\n",
    "    if not os.path.exists(class_dir):\n",
    "        print(f\"Error: Folder {class_dir} does not exist.\")\n",
    "        continue\n",
    "    files = glob.glob(f\"{class_dir}/*.jpg\")  # Adjust extension if needed\n",
    "    print(f\"Class: {class_name}, Files Found: {len(files)}\")  # Debug: Count files\n",
    "    for file_path in files:\n",
    "        image_paths.append(file_path)\n",
    "        labels.append(class_label)\n",
    "\n",
    "# Proceed with the pipeline if files are found\n",
    "if len(image_paths) == 0:\n",
    "    raise ValueError(\"No images found. Check dataset folder names or file paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [128, 128])\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize to [0, 1]\n",
    "    return image\n",
    "\n",
    "# Load Dataset\n",
    "def load_dataset(image_paths, labels):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    dataset = dataset.map(lambda x, y: (preprocess_image(x), y))\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing so all datasets contain image tensors (not paths)\n",
    "preprocessed_dataset = load_dataset(image_paths, labels)\n",
    "\n",
    "# Split by class\n",
    "class_2_dataset = preprocessed_dataset.filter(lambda img, label: tf.equal(label, 2)).shuffle(buffer_size=1000).take(15_000)\n",
    "class_3_dataset = preprocessed_dataset.filter(lambda img, label: tf.equal(label, 3))\n",
    "class_0_dataset = preprocessed_dataset.filter(lambda img, label: tf.equal(label, 0))\n",
    "class_1_dataset = preprocessed_dataset.filter(lambda img, label: tf.equal(label, 1))\n",
    "\n",
    "# Combine the datasets\n",
    "final_dataset = (class_2_dataset\n",
    "                 .concatenate(class_3_dataset)\n",
    "                 .concatenate(class_0_dataset)\n",
    "                 .concatenate(class_1_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_counts = Counter()\n",
    "\n",
    "# Loop through dataset\n",
    "for _, lbl in final_dataset:\n",
    "    class_index = int(lbl.numpy())  # Convert one-hot to class index\n",
    "    label_counts[class_index] += 1\n",
    "\n",
    "# Print class distribution\n",
    "print(\"Final dataset class distribution:\")\n",
    "for label, count in sorted(label_counts.items()):\n",
    "    print(f\"Class {label}: {count} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def save_dataset_to_folder(dataset, folder_path, class_names):\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    for class_name in class_names:\n",
    "        class_dir = os.path.join(folder_path, class_name)\n",
    "        os.makedirs(class_dir, exist_ok=True)\n",
    "\n",
    "    counter = [0] * len(class_names)  # To count how many images per class\n",
    "\n",
    "    for image, label in dataset:\n",
    "        # If label is one-hot encoded, convert to integer\n",
    "        if tf.rank(label) > 0:\n",
    "            label = tf.argmax(label, axis=-1)\n",
    "\n",
    "        print(label)\n",
    "        label = int(label.numpy())\n",
    "        class_name = class_names[label]\n",
    "\n",
    "        # Convert tensor to numpy and save as PNG\n",
    "        image_np = image.numpy()\n",
    "        if image_np.dtype != 'uint8':\n",
    "            image_np = (image_np * 255).astype('uint8')  # normalize if needed\n",
    "\n",
    "        img = Image.fromarray(image_np)\n",
    "        img_path = os.path.join(folder_path, class_name, f'{counter[label]}.jpg')\n",
    "        img.save(img_path)\n",
    "\n",
    "        counter[label] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_dataset_to_folder(final_dataset, \"final_dataset\", classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Paths\n",
    "dataset_dir = \"final_dataset\"  # Root folder containing class folders\n",
    "# Adjust class names to match actual folder names\n",
    "classes = [\"Mild Dementia\", \"Moderate Dementia\", \"Non Demented\", \"Very mild Dementia\"]\n",
    "\n",
    "# Load Data with Correct Folder Names\n",
    "image_paths, labels = [], []\n",
    "for class_label, class_name in enumerate(classes):\n",
    "    class_dir = os.path.join(dataset_dir, class_name)\n",
    "    if not os.path.exists(class_dir):\n",
    "        print(f\"Error: Folder {class_dir} does not exist.\")\n",
    "        continue\n",
    "    files = glob.glob(f\"{class_dir}/*.jpg\")  # Adjust extension if needed\n",
    "    print(f\"Class: {class_name}, Files Found: {len(files)}\")  # Debug: Count files\n",
    "    for file_path in files:\n",
    "        image_paths.append(file_path)\n",
    "        labels.append(class_label)\n",
    "\n",
    "# Proceed with the pipeline if files are found\n",
    "if len(image_paths) == 0:\n",
    "    raise ValueError(\"No images found. Check dataset folder names or file paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, testing and validation\n",
    "train_paths, test_paths, train_labels, test_labels = train_test_split(image_paths, labels, test_size=0.3, random_state=42,stratify=labels) # suffle by default and straity labels \n",
    "test_paths, val_paths, test_labels, val_labels = train_test_split(test_paths, test_labels, test_size=0.5, random_state=42,stratify=test_labels) # suffle by default and straity labels\n",
    "# to keep the same class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(train_paths, train_labels)\n",
    "val_dataset = load_dataset(val_paths, val_labels)\n",
    "test_dataset = load_dataset(test_paths, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset_to_folder(train_dataset, \"train_dataset\", classes)\n",
    "save_dataset_to_folder(val_dataset, \"val_dataset\", classes)\n",
    "save_dataset_to_folder(test_dataset, \"test_dataset\", classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: Mild Dementia, Files Found: 3501\n",
      "Class: Moderate Dementia, Files Found: 342\n",
      "Class: Non Demented, Files Found: 10500\n",
      "Class: Very mild Dementia, Files Found: 9607\n"
     ]
    }
   ],
   "source": [
    "# Define Paths\n",
    "dataset_dir = \"train_dataset\"  # Root folder containing class folders\n",
    "# Adjust class names to match actual folder names\n",
    "classes = [\"Mild Dementia\", \"Moderate Dementia\", \"Non Demented\", \"Very mild Dementia\"]\n",
    "\n",
    "# Load Data with Correct Folder Names\n",
    "image_paths, labels = [], []\n",
    "for class_label, class_name in enumerate(classes):\n",
    "    class_dir = os.path.join(dataset_dir, class_name)\n",
    "    if not os.path.exists(class_dir):\n",
    "        print(f\"Error: Folder {class_dir} does not exist.\")\n",
    "        continue\n",
    "    files = glob.glob(f\"{class_dir}/*.jpg\")  # Adjust extension if needed\n",
    "    print(f\"Class: {class_name}, Files Found: {len(files)}\")  # Debug: Count files\n",
    "    for file_path in files:\n",
    "        image_paths.append(file_path)\n",
    "        labels.append(class_label)\n",
    "\n",
    "# Proceed with the pipeline if files are found\n",
    "if len(image_paths) == 0:\n",
    "    raise ValueError(\"No images found. Check dataset folder names or file paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: {0: 3501, 1: 342, 2: 10500, 3: 9607}\n",
      "After SMOTE: {0: 10500, 1: 10500, 2: 10500, 3: 10500}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from PIL import Image\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Define your class labels\n",
    "classes = ['Non Demented', 'Very mild Dementia', 'Mild Dementia', 'Moderate Dementia']  # Update if needed\n",
    "\n",
    "# -------------------------\n",
    "# Image Preprocessing\n",
    "# -------------------------\n",
    "def preprocess_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [128, 128])  # Resize to save memory\n",
    "    image = tf.cast(image, tf.float16) / 255.0   # Normalize and use float16 to reduce RAM\n",
    "    return image\n",
    "\n",
    "def load_and_preprocess_images(image_paths):\n",
    "    images = [preprocess_image(path) for path in image_paths]\n",
    "    return tf.stack(images)  # shape: (n_samples, 128, 128, 3)\n",
    "\n",
    "# -------------------------\n",
    "# Load & Prepare Dataset\n",
    "# -------------------------\n",
    "X = load_and_preprocess_images(image_paths)\n",
    "X = tf.reshape(X, [X.shape[0], -1])  # Flatten for SMOTE\n",
    "X = X.numpy()  # SMOTE needs NumPy\n",
    "\n",
    "y = np.array(labels)\n",
    "\n",
    "# -------------------------\n",
    "# Apply SMOTE\n",
    "# -------------------------\n",
    "print(\"Before SMOTE:\", dict(zip(*np.unique(y, return_counts=True))))\n",
    "smote = SMOTE(random_state=42)\n",
    "X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "print(\"After SMOTE:\", dict(zip(*np.unique(y_balanced, return_counts=True))))\n",
    "\n",
    "# -------------------------\n",
    "# Reshape back to images\n",
    "# -------------------------\n",
    "X_balanced = X_balanced.reshape(-1, 128, 128, 3)\n",
    "X_balanced = np.clip(X_balanced, 0, 1)  # Ensure pixel values are in [0,1]\n",
    "\n",
    "# Create TensorFlow Dataset\n",
    "smote_dataset = tf.data.Dataset.from_tensor_slices((X_balanced.astype(np.float32), y_balanced))\n",
    "\n",
    "# -------------------------\n",
    "# Batched Saving Function\n",
    "# -------------------------\n",
    "def save_dataset_to_folder_batched(dataset, folder_path, class_names, batch_size=128):\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    for class_name in class_names:\n",
    "        os.makedirs(os.path.join(folder_path, class_name), exist_ok=True)\n",
    "\n",
    "    counter = [0] * len(class_names)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    for batch in dataset:\n",
    "        images, labels = batch\n",
    "        for image, label in zip(images, labels):\n",
    "            label = int(label.numpy())\n",
    "            class_name = class_names[label]\n",
    "\n",
    "            image_np = (image.numpy() * 255).astype('uint8')\n",
    "            img = Image.fromarray(image_np)\n",
    "\n",
    "            img_path = os.path.join(folder_path, class_name, f'{counter[label]}.jpg')\n",
    "            img.save(img_path)\n",
    "            counter[label] += 1\n",
    "\n",
    "        del images, labels\n",
    "        gc.collect()\n",
    "\n",
    "# -------------------------\n",
    "# Save to Disk (Batched)\n",
    "# -------------------------\n",
    "save_dataset_to_folder_batched(smote_dataset, \"smote_dataset\", classes, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Image preprocessing functions\n",
    "def preprocess_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [128, 128])\n",
    "    image = tf.cast(image, tf.float16) / 255.0\n",
    "    return image\n",
    "\n",
    "def load_and_preprocess_images(image_paths):\n",
    "    images = [preprocess_image(path) for path in image_paths]\n",
    "    return tf.stack(images)\n",
    "\n",
    "# Load and preprocess image data\n",
    "X = load_and_preprocess_images(image_paths)  # shape: (n_samples, 128, 128, 3)\n",
    "X = tf.reshape(X, [X.shape[0], -1])  # Flatten to (n_samples, features)\n",
    "X = X.numpy()  # Convert to NumPy for SMOTE\n",
    "\n",
    "# Convert labels to NumPy array\n",
    "y = np.array(labels)\n",
    "\n",
    "# Apply SMOTE directly\n",
    "smote = SMOTE(random_state=42)\n",
    "X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "\n",
    "print(\"Before SMOTE:\", dict(zip(*np.unique(y, return_counts=True))))\n",
    "print(\"After SMOTE:\", dict(zip(*np.unique(y_balanced, return_counts=True))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape flattened vectors back to (128, 128, 3)\n",
    "X_balanced = X_balanced.reshape(-1, 128, 128, 3)\n",
    "\n",
    "# Optional: make sure values are in [0, 1] range (if SMOTE messed with them)\n",
    "X_balanced = np.clip(X_balanced, 0, 1)\n",
    "\n",
    "# Create the TensorFlow dataset\n",
    "smote_dataset = tf.data.Dataset.from_tensor_slices((X_balanced.astype(np.float32), y_balanced))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset_to_folder_batched(dataset, folder_path, class_names, batch_size=128):\n",
    "    import gc\n",
    "\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    for class_name in class_names:\n",
    "        os.makedirs(os.path.join(folder_path, class_name), exist_ok=True)\n",
    "\n",
    "    counter = [0] * len(class_names)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    for batch in dataset:\n",
    "        images, labels = batch\n",
    "        for image, label in zip(images, labels):\n",
    "            label = int(label.numpy())\n",
    "            class_name = class_names[label]\n",
    "\n",
    "            image_np = (image.numpy() * 255).astype('uint8')\n",
    "            img = Image.fromarray(image_np)\n",
    "\n",
    "            img_path = os.path.join(folder_path, class_name, f'{counter[label]}.jpg')\n",
    "            img.save(img_path)\n",
    "            counter[label] += 1\n",
    "\n",
    "        # 🧹 Clear memory after each batch\n",
    "        del images, labels\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "save_dataset_to_folder_batched(smote_dataset, \"smote_dataset\", classes, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Agumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: Mild Dementia, Files Found: 10500\n",
      "Class: Moderate Dementia, Files Found: 10500\n",
      "Class: Non Demented, Files Found: 10500\n",
      "Class: Very mild Dementia, Files Found: 10500\n"
     ]
    }
   ],
   "source": [
    "# Define Paths\n",
    "dataset_dir = \"smote_dataset\"  # Root folder containing class folders\n",
    "# Adjust class names to match actual folder names\n",
    "classes = [\"Mild Dementia\", \"Moderate Dementia\", \"Non Demented\", \"Very mild Dementia\"]\n",
    "\n",
    "# Load Data with Correct Folder Names\n",
    "image_paths, labels = [], []\n",
    "for class_label, class_name in enumerate(classes):\n",
    "    class_dir = os.path.join(dataset_dir, class_name)\n",
    "    if not os.path.exists(class_dir):\n",
    "        print(f\"Error: Folder {class_dir} does not exist.\")\n",
    "        continue\n",
    "    files = glob.glob(f\"{class_dir}/*.jpg\")  # Adjust extension if needed\n",
    "    print(f\"Class: {class_name}, Files Found: {len(files)}\")  # Debug: Count files\n",
    "    for file_path in files:\n",
    "        image_paths.append(file_path)\n",
    "        labels.append(class_label)\n",
    "\n",
    "# Proceed with the pipeline if files are found\n",
    "if len(image_paths) == 0:\n",
    "    raise ValueError(\"No images found. Check dataset folder names or file paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = \"smote_dataset\"\n",
    "path_val = \"val_dataset\"\n",
    "#Rescale data and create data generator instances\n",
    "val_datagenerator = ImageDataGenerator(rescale=1/255.)\n",
    "train_datagenerator_augmentation = ImageDataGenerator(rescale = 1/255.,\n",
    "                                                      rotation_range=20, #rotate the image\n",
    "                                                      zoom_range = 0.2,#zoom the image\n",
    "                                                      width_shift_range=0.2, #shift the image horizontally\n",
    "                                                      height_shift_range=0.2, #shift the image vertically\n",
    "                                                      horizontal_flip=True, #flip the image on horizontal axis\n",
    "                                                      vertical_flip=True, #flip the image on vertical axis\n",
    "                                                      shear_range = 0.2) #Shear the image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5133 images belonging to 4 classes.\n",
      "Found 42000 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "val_data = val_datagenerator.flow_from_directory(path_val,\n",
    "                                                     target_size=(128,128),\n",
    "                                                     batch_size=32,\n",
    "                                                     class_mode='categorical'\n",
    "                                                    )\n",
    "train_data_augmented = train_datagenerator_augmentation.flow_from_directory(path_train,\n",
    "                                                                            target_size=(128,128),\n",
    "                                                                            batch_size=32,\n",
    "                                                                            class_mode='categorical',\n",
    "                                                                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "def create_cnn(num_classes=4):\n",
    "    model = keras.Sequential([\n",
    "        # Convolutional Block 1\n",
    "        layers.Conv2D(32, (3,3), activation='relu', input_shape=(128, 128, 3)),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        # Convolutional Block 2\n",
    "        layers.Conv2D(64, (3,3), activation='relu'),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        # Convolutional Block 3\n",
    "        layers.Conv2D(128, (3,3), activation='relu'),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        # Flatten & Dense Layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),  # Reduce overfitting\n",
    "        layers.Dense(num_classes, activation='softmax')  # Output layer\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        # loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "cnn_model = create_cnn()\n",
    "\n",
    "# Print model summary\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "cnn_model.fit(train_data_augmented, validation_data=val_data, epochs=300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Model\n",
    "cnn_model.save(\"alzheimers_detection_model_FJSL.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(history):\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "    epochs = range(len(history.history['loss']))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.plot(epochs, loss, label='training_loss')\n",
    "    plt.plot(epochs, val_loss, label='val_loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, accuracy, label='training_accuracy')\n",
    "    plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(cnn_model.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "# Load the model\n",
    "cnn_model = load_model(\"Modelos/FJSL_Alzheimer_Classification_V2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Paths\n",
    "dataset_dir = \"val_dataset\"  # Root folder containing class folders\n",
    "# Adjust class names to match actual folder names\n",
    "classes = [\"Mild Dementia\", \"Moderate Dementia\", \"Non Demented\", \"Very mild Dementia\"]\n",
    "\n",
    "# Load Data with Correct Folder Names\n",
    "image_paths, labels = [], []\n",
    "for class_label, class_name in enumerate(classes):\n",
    "    class_dir = os.path.join(dataset_dir, class_name)\n",
    "    if not os.path.exists(class_dir):\n",
    "        print(f\"Error: Folder {class_dir} does not exist.\")\n",
    "        continue\n",
    "    files = glob.glob(f\"{class_dir}/*.jpg\")  # Adjust extension if needed\n",
    "    print(f\"Class: {class_name}, Files Found: {len(files)}\")  # Debug: Count files\n",
    "    for file_path in files:\n",
    "        image_paths.append(file_path)\n",
    "        labels.append(class_label)\n",
    "\n",
    "# Proceed with the pipeline if files are found\n",
    "if len(image_paths) == 0:\n",
    "    raise ValueError(\"No images found. Check dataset folder names or file paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    image = tf.io.read_file(image_path)  # Read the image from the path\n",
    "    image = tf.image.decode_jpeg(image, channels=3)  # Decode the image to RGB\n",
    "    image = tf.image.resize(image, [128, 128])  # Resize to (128, 128)\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize to [0, 1]\n",
    "    return image\n",
    "\n",
    "# Load and preprocess a list of images\n",
    "def load_and_preprocess_images(image_paths):\n",
    "    # Preprocess all images and store them in a list\n",
    "    images = [preprocess_image(image_path) for image_path in image_paths]\n",
    "    \n",
    "    # Stack the images into a single batch (shape: (batch_size, 128, 128, 3))\n",
    "    images_batch = tf.stack(images)\n",
    "    \n",
    "    return images_batch\n",
    "\n",
    "\n",
    "test_data = load_and_preprocess_images(image_paths)\n",
    "\n",
    "# Convert labels to NumPy array\n",
    "labels = np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming y_true is the actual labels (e.g., integer labels)\n",
    "y_true = labels  # Actual labels for the test set\n",
    "predictions = cnn_model.predict(test_data)\n",
    "y_pred = np.argmax(predictions, axis=1)  # Convert probabilities to class labels\n",
    "\n",
    "print(\"True Labels:\", y_true)\n",
    "print(\"Predicted Labels:\", y_pred)\n",
    "\n",
    "# Classification Report\n",
    "class_names = classes  # List of class names\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AAUTIA2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
